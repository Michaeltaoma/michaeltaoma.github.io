---
layout: post
updated: 2020-05-24 15:19
---

### Deep Feedforward Networks
- Deep feedforward networks, feedforward neural networks, Multilayer perceptrons其实都是一个东西。目的都是学的一个方程 \\( f\* \\)比如可以用来学得一个把数据x映射到类别y的分类器。
-  *A feedforward network defines a mapping \\( y = f(x;\theta) \\) and learns the value of the parameters \\( \theta \\) that result in the best function approximation* 
- feedforward并没有feedback connection，有feedback connection的叫做recurrent network.
- 用数学的方式来表示layer：\\( f(x) = f^{(3)}(f^{(2)}(f^{(1)}(x))) \\)，那么可以看到在这个例子里面，（1）就代表的是第一个layer，（2）代表的就是第二个layer。这个chain的长度就是模型的深度depth。
- 除了output layer，并没有额外的信息来告诉这个模型在每个hidden layer他应该如何表现
- 刚刚说到了*depth*深度，而hidden layer的维度则决定了模型的宽度
- It is best to think offeedforward networks as function approximation machines that are designed toachieve statistical aeneralization, occasionally drawing some insights from what weknow about the brain, rather than as models of brain function
#### Linear Models
- Logistic regression，linear regression
- 使用他们的好处是：
	- Can be fit efficiently and reliably. either in closed form or with convex optimization
- 缺点就是线性模型只可以model线性函数，也就是说这种方式并不可以找出任何的关系
- 解决的办法是把数据x映射到\\( \phi(x) \\)，\\( \phi(x) \\)指的是x的一个新的representation，线性模型是可以作用在这个新的x上面
- 如何找到这个\\( \phi () \\)映射呢？
	- 把x映射到无限维，就像之前在自学452的时候说的那样
	- 手扒这个映射
	- 把这个\\( \phi \\)定义成一个hidden layer：\\( y = f(x;\theta, w) = \phi(x;\theta)^{T} w \\)。在这个方法下面，只需要先指明映射的大类，然后就可以让算法自己去学这个具体的映射

#### 6.1 Example: Learning XOR
- 数据集= {[0, 0], [0, 1], [1, 0], [1, 1]}
- 损失函数loss function：Mean Square Error：

	$$
	\begin{aligned}

	J(\theta) = \frac{1}{4} \sum_{x \in 数据集}(f^*(x) - f(x;\theta))^{2}

	\end{aligned}
	$$

- **所以在这些表示里面，\\( \theta \\)代表的是这一层级中的所有参数，那么在这里面的theta就有权重和bias，因为这个计算只是“一层的计算”。而像刚刚\\( y = f(x;\theta, w) = \phi(x;\theta)^{T} w \\)这个下面，为什么不把theta和w分在一起的原因是因为theta是属于那个映射的参数，而w是属于映射好了之后线性映射的参数，所以是不一样的**
- 不管怎么说，现在的模型如下：
	
	$$
	\begin{aligned}
	& f(x;w,b) = x^T w + b
	\end{aligned}
	$$
- 这篇文章提出的一些看法是之前从来没讲过的，还是觉得把这么重要的一门课拿来给farhana这种划水教授讲真的败坏老子的前程。
- 下面两张图分别代表了现在的problem space和他propose的多层前置网络

	<img src="assets/post_pics/Snipaste_2020-05-24_16-33-16.png">

	- 观察右边的h sapce，他把(0, 1)和(1, 0)这两个点都映射到了同一个点(1, 0)上面，这样就线性可分了

	<img src="assets/post_pics/Snipaste_2020-05-24_16-34-48.png" width="500">

- 他这里关于激活函数的使用就和之前理解的很不一样。之前理解的是，激活函数表现的就像是神经元的携带信息的动作电位，而这里的解释是激活函数**把线性不可分的数值等价的映射到线性可分的位置上面**
- **隐藏层的激活函数选用线性函数的意义不大，如果隐藏层依然选用线性函数的话，整体的模型也还是一个线性函数**

	$$
	\begin{aligned}
	& h = g(W^T x + c)
	\end{aligned}
	$$

- 以上的g代表的就是一个激活函数，在这里他选用的是ReLu（Rectified Linear Unit）线性整流函数（修正线性单元）
	
	$$
	f(x) = max(0, x) 
	$$

- 那么这样整合下来的话，完整的模型就是:
	
	$$
	\begin{aligned}
	f(x;W, c, w, b) = w^{T} max(0, W^{T} x + c) + b
	\end{aligned}
	$$

- 书中还演示了一遍walk through，这个walkthrough是根据一个已有的答案来的，这个答案属于global minimum of loss function set里面，也就是说还有别的答案

### 6.2 Gradient-Based Learning
- Convex and loss function: Popular loss functions are convex because a local minimum of a convex function is a global minimum. 对于一个convex的loss function来说，我们的目标是找到这个loss function的convex的local minimum，这个local minimum只有一个且是global minimum。所以loss function如果是convex的话，对于之后找极值相当有帮助。
- 所以训练神经网络和线性模型最大的区别就是：
	- 线性模型使用convex optimization凸优化，而凸优化从任何initial parameters开始，只要找到了一局部最优，因为我们的损失函数是凸函数，那么也就找到了这个函数的全剧最优，所以“一旦将一个实际问题表述为凸优化问题，大体上意味着相应问题已经得到彻底解决，这是非凸的优化问题所不具有的性质。”
	- 神经网络模型使用的是随机梯度下降，使用这个的原因是因为神经网络的损失函数往往是非凸函数，非凸函数就不存在局部最优必定等于全局最优，所以神经网络的优化器只可以随机开始于损失函数的一点，然后慢慢一步一步找全剧最优（沿着梯度找）
		- 梯度：梯度的本意是一个向量（矢量），表示某一函数在该点处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向（此梯度的方向）变化最快，变化率最大（为该梯度的模）。
		- 所以其实对于神经网络的优化器来说，initial condition还挺重要的
- **the training algorithmis almost always based on using the gradient to descend the cost function in oneway or another.**

#### 6.2.1 Cost function
- Cost function一般是这样定义的：In most cases, our parametric model deﬁnes a distribution \\( p(y|x;\theta) \\) and we simply use the principle of maximum likelihood
	- Principle of maximum likelihood:
		- 简单来说就是我们选择模型是基于概率的：我们选择模型的原则是，他比所有别的模型都更好，给定样本的情况下，参数是某参数的概率是最高的，也就是参数基于样本的后验概率是最高的，以下是一个最大似然估计（Maximum Likelihood Estimation）的例子
			假设有个袋子，里面有7个白球和三个红球，设白球的比例为\\( \theta \\)

			$$
			\begin{aligned}
			f(x_{1}, x_{2} | \theta) = f(x_{1}|\theta) \times f(x_2 | \theta)
			\end{aligned}
			$$
			
			其中x1和x2表示两次采样，f是模型而theta是模型的参数，因为这两个样本是独立同分布，所以他们一起发生的概率就是相乘在一起，现在模型参数theta是未知的，为了求得这个theta，我们把似然L定义成theta关于采样的后验概率
			
			$$
			\begin{aligned}
			L(\theta|x_1, x_2) = f(x_1, x_2 | \theta) = \prod_{i = 1}^{2} f(x_i | \theta)
			\end{aligned}
			$$

		- 所以接下来的目的，就是找到令似然函数最大的theta值
- 像刚刚的例子他定义的分布就很简单就是一个概率分布，而在机器学习中，如果把似然和概率分布的概念利用起来的话，就可以得到下面的式子	
- 其中，\\( P_{model}(x;\theta) \\)是：maps any configuration x to a real number estimating the true probabolity
- 然后\\( P_{model}(x) \\)也等同于p(y|x;theta)
$$
\begin{aligned}
\theta_{ML} = argmax_{\theta} \prod_{i = 1}^{m} P_{model}(x^{(i)};\theta)
\end{aligned}
$$

- 意思就是给定参数和模型下面，预测出来的类别是这个y的概率是多少，那么为什么有的时候说**log p**呢，这是因为，如果两边同时取对数的话，不影响函数的单调性也不影响选出来的theta值，但可以把连乘变成连加，大大减少了之后求导的难度

$$
\begin{aligned}
\theta_{ML} = argmax_{\theta} \sum_{i = 1}^{m} LogP_{model}(x^{(i)};\theta)
\end{aligned}
$$

- 所以我们训练theta值的时候这么训练的:

$$
\begin{aligned}
\theta_{ML} = argmax_{\theta} E_{x \sim \hat{P}_{data}}[LogP_{model}(x^{(i)};\theta)]
\end{aligned}
$$


##### 6.2.1.1 Learning Conditional Distributions with Maximum Likelihood

$$
\begin{aligned}
J(\theta) = -E_{x, y \sim \hat{P}_{data}} [Log p_{model}(y|x)]
\end{aligned}
$$

- 以上就是一个cost function，适用于各种模型，根据不同log p模型的选择，最终出来的cost function也不一样。打个比方如果是linear regression的话，他代入的结果就是一个mean suqare error cost
- 这个的好处是，因为需要用到梯度，我们的代价函数需要large and predictable enough才可以成为一个很好的指路人，求得的梯度才有意义。但是如果一些非常饱和的代价函数（饱和的代价函数指的是在图像上非常平的代价函数）会破坏这种算法，也不能说是破坏吧，总而言之就是会令算法的效率非常低并且可能会陷入局部最优。造成这一问题的原因是有些激活函数在遇到极负的参数会变得饱和：**几个输出单元都涉及到一个exp函数，当它的参数非常负面时，这个函数可能会达到饱和。负对数似然代价函数中的对数函数抵消了某些输出单元的输出.**
	- 这个抵消就很关键：\\( Log_{e} e^{X} = X \\)
- 对于实值输出变量，如果模型能够控制输出分布的密度(例如，通过学习高斯输出分布的变异参数)，那么就有可能将极高的密度分配给正确的训练集输出，从而导致交叉熵趋近于负无穷。第7章描述的正则化技术提供了几种修改学习问题的不同方法，这样模型就不能以这种方式获得无限的回报（完全没看懂的一句话）

##### 6.2.1.2 Learning Conditional Statistics
- Instead of learning a full probability distribution \\( p(y|x;\theta) \\), we often want to learn just one conditional statistic of y given x. 比如，我们可能只想知道我们训练出来的y的平均值
- 这个东西就很牛逼，如果我们把神经网络理解成：神经网络可以从许许多多的函数集合中找到对应的函数（这些函数的集合是很泛的）。那么可以把成本函数看成是一个**functional泛函数: A functional is a mapping from functions to real numbers.**这个就很高级了，学习不再是调参，而是寻找函数，我们可以令损失函数的最低点为一个我们想要的函数。比如我们可以令损失函数的最低点落在一个把x映射到y的expected value的函数上。这种优化问题被称为：**Calculus of variation**

<img src="assets/post_pics/Snipaste_2020-05-25_17-25-58.png">

- 根据变分法，其实上面6.14的公式也很好看懂，也就是通过求解这个最优问题（以minimize f为目标）来找到一个计算y的均值的式子。而得出来的计算平均值的函数属于：**mean absolute error**

#### 6.2.2 Output Units
- 损失函数和表示output unit的选择是深深耦合在一起的。
- 事实上，output unit和hidden unit作为神经网络的unit其实是没什么区别的，只不过output unit处于output layer并且负责output。
- 在这一个小节的学习中，hidden feature表示为：\\( h = f(x;\theta) \\)，而output layer则为这些特征增加了最后一层的变换

##### 6.2.2.1 Linear Units for Gaussian Output Distribution
- 顾名思义：如果是linear unit的话，他的output layer会预测出\\( \hat{y} = W^{T}h + b \\)

- Linear output layers are often used to produce the mean of a conditional gaussian distribution 

$$
\begin{aligned}
	p(y|x) = \mathcal{N} (y;\hat{y}, I)
\end{aligned}
$$

- 这里假设这个后验概率是高斯分布的极其重要，因为对于高斯分布的话：
	
	$$
	\begin{aligned}
		p(x) = \frac{1}{\sqrt{2\pi} {\sigma}} e^{-\frac{1}{2{\sigma}^{2}} x^{2}}
	\end{aligned}
	$$

- 这个性质也非常关键，可以拿来推mean square error，现在定义一个似然函数：
	
	$$
	\begin{aligned}
	& L(\theta) = \prod_{i = 1}^{n} p(y|x, \theta, \sigma) \\
	& 给两边取对数，方便计算 \\
	& J(\theta) = nln \frac{1}{\sqrt{2\pi}\sigma} - \frac{1}{2{\sigma}^{2}} \sum_{i = 1}{n}(y - f^{*}(x))^2 
	\end{aligned}
	$$

- 所以本质上，线性拟合找的是结果y的条件高斯分布的mean，然后根据最大似然的理念去得到一个代价函数，这个代价函数等同于最小二乘
- linear units do not saturate, they pose little diﬃculty for gradient-based optimization algorithms and may be used with a wide variety of optimizationalgorithms从不饱和，何时是个头

##### 6.2.2.2 Sigmoid Units for Bernoulli Output Distribution
- 伯努利分布指的是对于随机变量X有, 参数为p(0<p<1)，如果它分别以概率p和1-p取1和0为值。EX= p,DX=p(1-p)。伯努利试验成功的次数服从伯努利分布,参数p是试验成功的概率。伯努利分布是一个离散型机率分布，是N=1时二项分布的特殊情况，为纪念瑞士科学家詹姆斯·伯努利(Jacob Bernoulli 或James Bernoulli)而命名。
- 如果用这一个的话，因为probability需要定义在[0, 1]这个范围内，所以如果是用linear的话，如果approximate出来的东西的梯度会变成0（具体可以看书上的例子），这样子梯度是0的东西就无法对他的参数进行调整
- 综上，我们用sigmoid output unit来代替linear output unit：\\( \hat{y} = \sigma (w^{T}h + b) \\), 在原有的linear output的基础上多加一个sigmoid函数去把linear output z变成一个概率
- 然后书上propose了一个推理过程（how to deﬁne aprobability distribution overyusing the value z）：
	- 假设存在一个没有normalized（没有把数据映射到[0, 1]）的概率分布 \\( \hat{P}(y) \\)
	- 再假设这个概率分布取对数之后是和y和z成线性关系：\\( log \hat{P}(y) = yz \\)
	- 分别取e，消除ln：\\( \hat{P}(y) = exp(yz) \\)
	- 重新normalize，把这个概率变成一个恰当的概率分布：\\( P(y) = \frac{exp(yz)}{\sum_{y' = 0}^{1} exp(y'z)} \\)
	- 观察发现这不就是先linear output再取sigmoid嘛！
- The z variable deﬁning such a distribution over binary variables is called a logit

	<img src="assets/post_pics/Snipaste_2020-05-25_19-28-01.png">

- 最后是一个softplus function，softplus function的定义如下：

$$
\begin{aligned}
f(x) = ln(1 + e^{x})
\end{aligned}
$$

- 这个的好处是在训练的时候，只有当(1-2y)z非常负->也就是点那个y = 1然后z非常正，也就是正确答案的时候，此训练才会饱和
- 那如果是一个很错误的预测呢？-> The derivative with respect tozasymptotes tosign(z), so, in the limit of extremelyincorrectz, the softplus function does not shrink the gradient at all. This propertyis useful because it means that gradient-based learning can act to quickly correcta mistaken z.
- 还是那句话，因为有sigmoid的存在，函数有时候太容易饱和了，所以两边取个对数就舒服很多
